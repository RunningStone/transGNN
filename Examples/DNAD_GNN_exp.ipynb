{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loc = \"\"\n",
    "label_loc = \"\"\n",
    "ckpt_loc = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transGNN.Data.paras import ReaderParas\n",
    "reader_para = ReaderParas()\n",
    "reader_para.data_file_loc = data_loc\n",
    "reader_para.data_pid = 'PatientID'\n",
    "reader_para.label_file_loc = label_loc\n",
    "reader_para.label_pid = 'PatientID'\n",
    "reader_para.label_name = 'HRD'\n",
    "reader_para.label_dict = {'HRD':1, 'HR-proficient':0}\n",
    "reader_para.with_transpose = False\n",
    "reader_para.gene_thresh = None\n",
    "reader_para.adj_thresh = 0.1\n",
    "\n",
    "reader_para.test_ratio = 0.01\n",
    "reader_para.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transGNN.Data.dataset import create_dataset\n",
    "#train_loader,test_loader,trainset,testset,reader,class_weights = create_dataset(reader_para)\n",
    "import torch\n",
    "from transGNN.Data.paras import ReaderParas\n",
    "from transGNN.Data.preprocessing import DataReader\n",
    "from torch.utils.data import random_split,TensorDataset,DataLoader\n",
    "\n",
    "reader = DataReader(reader_para) \n",
    "features_table, labels_str = reader.data_readfiles()\n",
    "reader.get_adj_M() # get adj matrix\n",
    "\n",
    "full_dataset = TensorDataset(features_table, labels_str)\n",
    "\n",
    "test_size = int(reader_para.test_ratio * len(full_dataset))\n",
    "train_size = len(full_dataset) - test_size\n",
    "\n",
    "# split dataset with random or K-fold\n",
    "trainset,testset = random_split(full_dataset,\n",
    "                lengths=[train_size,test_size],\n",
    "                generator=torch.Generator().manual_seed(0)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# random weighted sampler \n",
    "# sampler\n",
    "\n",
    "batch_size = reader_para.batch_size\n",
    "\n",
    "\n",
    "from torch.utils.data.sampler import WeightedRandomSampler\n",
    "# \n",
    "# \n",
    "num_classes = len(reader_para.label_dict.keys())\n",
    "# \n",
    "class_counts = [0] * num_classes  #\n",
    "for _, label in trainset:\n",
    "    class_counts[int(label)] += 1\n",
    "\n",
    "# \n",
    "class_weights = [len(trainset) / class_counts[i] for i in range(num_classes)]\n",
    "\n",
    "# \n",
    "weights = [class_weights[int(label)] for _, label in trainset]\n",
    "\n",
    "trainsampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "#weights = [1.0 / len(testset) for i in range(len(testset))]\n",
    "#testsampler = WeightedRandomSampler(weights, len(weights))\n",
    "\n",
    "# \n",
    "train_loader = DataLoader(dataset=trainset, \n",
    "                                batch_size=batch_size,sampler=trainsampler) #shuffle=True)\n",
    "test_loader = DataLoader(dataset=testset, \n",
    "                                batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reader.paras.label_dict,class_counts)\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transGNN.Model.GAT.paras import GATParas\n",
    "from transGNN.Predefine.GAT_classifier import gat_model_trainer_para\n",
    "from transGNN.Model.GAT.pl import pl_GAT\n",
    "gat_model_trainer_para.project = \"DNAD_GNN_GAT\" # project name\n",
    "gat_model_trainer_para.entity= \"shipan_work\" # entity name\n",
    "gat_model_trainer_para.exp_name = \"GNN_GAT\" # experiment name\n",
    "gat_model_trainer_para.class_nb = len(reader.paras.label_dict.keys())\n",
    "gat_model_trainer_para.batch_size = reader_para.batch_size\n",
    "\n",
    "gat_model_trainer_para.optimizers:list = [torch.optim.Adam,\n",
    "                       ] # list of optimizer\n",
    "gat_model_trainer_para.optimizer_paras:list =[\n",
    "        {\"lr\":1e-4},# for first optimizer\n",
    "    ] # list of optimizer paras dict\n",
    "\"\"\"\n",
    "gat_model_trainer_para.schedulers:list = [torch.optim.lr_scheduler.StepLR] # list of scheduler\n",
    "gat_model_trainer_para.scheduler_paras:list = [{\n",
    "                            #\"decay_step\":10,\n",
    "                            \"step_size\":800,\n",
    "                            \"gamma\":0.5},] # list of scheduler paras dict\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "gat_model_trainer_para.save_ckpt = True # save checkpoint or not\n",
    "gat_model_trainer_para.ckpt_folder = \"/Users/shipan/Documents/workspace_transGNN/pretrained/\"\n",
    "#debug: try to add formated name to ckpt\n",
    "gat_model_trainer_para.ckpt_format:str = \"_{epoch:02d}-{auroc_train:.2f}\" # check_point format \n",
    "gat_model_trainer_para.ckpt_para = { #-----------> paras for pytorch_lightning.callbacks.ModelCheckpoint\n",
    "                    \"save_top_k\":1,\n",
    "                   \"monitor\":\"auroc_train\",\n",
    "                   \"mode\":\"max\",}\n",
    "                   \n",
    "gat_model_trainer_para.with_logger = \"wandb\" # \"wandb\",\n",
    "gat_model_trainer_para.wandb_api_key = \"Your wandb api key\"\n",
    "\n",
    "gat_model_trainer_para.max_epochs = 40 # max epochs\n",
    "\n",
    "model_paras = GATParas()\n",
    "model_paras.input_dim = 1\n",
    "model_paras.lin_input_dim=681\n",
    "model_paras.is_fixed_adj_matrix = True\n",
    "pl_model = pl_GAT(trainer_paras=gat_model_trainer_para,model_paras=model_paras)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_model.create_model()\n",
    "pl_model.set_adj_M(reader.M)\n",
    "\n",
    "pl_model.build_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_model.loss_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pl_model.trainer.fit(pl_model,train_loader,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load checkpoint for pytorch lightning model\n",
    "import torch\n",
    "pl_model.load_state_dict(torch.load(ckpt_loc,map_location=torch.device('cpu'))[\"state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "########################################################################\n",
    "# .    model requires gradient calculation functions for XAI\n",
    "########################################################################\n",
    "def calc_GAT_gradient(data,model,class_nb=2,concat_nb=3):\n",
    "    \"\"\"\n",
    "    in:\n",
    "        data: pyg_tensor with x and y\n",
    "        model: pytorch model\n",
    "    \n",
    "    out:\n",
    "        gradient: saved gradient for y_c = torch.sum(one_hot_labels*out)\n",
    "        feature_importance: feature importance for current data \n",
    "    \"\"\"\n",
    "    grad_labels = data.y.to(torch.int64)\n",
    "\n",
    "    result_dict=model(data)\n",
    "\n",
    "    GAT_features = result_dict[\"GAT_features\"]\n",
    "    out = result_dict[\"logits\"]\n",
    "\n",
    "    # clean gradient\n",
    "    GAT_features.grad = None\n",
    "    GAT_features.retain_grad()\n",
    "    \n",
    "    # to one hot\n",
    "    one_hot_labels = F.one_hot(grad_labels, num_classes=class_nb)\n",
    "    one_hot_labels = one_hot_labels.to(GAT_features.device)\n",
    "    y_c = torch.sum(one_hot_labels*out)\n",
    "\n",
    "    y_c.backward(retain_graph=True)\n",
    "    np_GAT_f_grad = GAT_features.grad.detach().cpu().numpy()\n",
    "    np_GAT_f_grad = np_GAT_f_grad.reshape([1,int(np_GAT_f_grad.shape[0]/concat_nb),concat_nb])\n",
    "    #print(np_GAT_f_grad.shape)\n",
    "    gradients = np.maximum(np_GAT_f_grad, 0)# (1, feature_size,concat_nb)\n",
    "    feature_importance = np.mean(gradients, 0)\n",
    "\n",
    "    return gradients,feature_importance\n",
    "\n",
    "def class_importance(target_class,labels,gradients,concat_nb,feature_dim):\n",
    "    \"\"\"\n",
    "    in:\n",
    "        target_class:int: target class for importance\n",
    "        labels:np.ndarray: a list of number as labels\n",
    "        gradients:\n",
    "    return:\n",
    "        class_i_importance:np.ndarray: importance score for class i\n",
    "    \"\"\"\n",
    "    class_i_index = np.argwhere(labels == target_class)\n",
    "    class_i_gradients = gradients[class_i_index[:, 0], :]\n",
    "    class_i_importance = np.mean(class_i_gradients, axis=0)\n",
    "    class_i_importance = np.reshape(class_i_importance, (concat_nb, feature_dim)).T\n",
    "    from sklearn.preprocessing import normalize\n",
    "    #print(class_i_importance.shape)\n",
    "    norm_class_i_importance = normalize(class_i_importance, axis=0, norm='max')\n",
    "    #print(norm_class_i_importance.shape)\n",
    "    class_i_importance = np.expand_dims(norm_class_i_importance, axis=0)\n",
    "\n",
    "    return class_i_importance #[1, feature_dim, concat_nb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_importance(dataloader,pl_model,reader):\n",
    "    gradient_matrixs=[]\n",
    "    labels = []\n",
    "    class_nb = len(reader.paras.label_dict.keys())\n",
    "    concat_nb = 3 \n",
    "    for _,(x,y) in enumerate(dataloader):\n",
    "        data = pl_model.get_graph_data(pl_model.M,x,y)\n",
    "        # not pl_model need pytorch model forward\n",
    "        gradients,_=calc_GAT_gradient(data,pl_model.model,\n",
    "                                        class_nb=class_nb,concat_nb=concat_nb) #G = [1,feat_dim,cat_nb]\n",
    "        gradient_matrixs.append(gradients)\n",
    "        labels.append(y)\n",
    "    importance_list = []\n",
    "    for i in range(class_nb):\n",
    "        imp_i = class_importance(target_class=i,\n",
    "                        labels=np.array(labels),\n",
    "                        gradients=np.array(gradient_matrixs),\n",
    "                        concat_nb=concat_nb,\n",
    "                        feature_dim=gradient_matrixs[0].shape[-2])\n",
    "        importance_list.append(imp_i)\n",
    "\n",
    "    np_imps = np.concatenate(importance_list,axis=0)\n",
    "    #layer_avg=np.mean(np_imps, axis=-1, keepdims=True)\n",
    "    layer_avg = np.sum(np_imps, axis=-1, keepdims=True)\n",
    "    def normalization(data):\n",
    "        _range = np.max(data) - np.min(data)\n",
    "        return (data - np.min(data)) / _range\n",
    "    layer_avg[0,...] = normalization(layer_avg[0,...])\n",
    "    layer_avg[1,...] = normalization(layer_avg[1,...])\n",
    "    np_imps_all = np.concatenate((np_imps, layer_avg), axis=-1)# [class_nb,feat_dim,concat_nb+1]\n",
    "    return np_imps_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_imps_all = infer_importance(train_loader,pl_model,reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[class_nb,feat_dim,concat_nb+1]\n",
    "np_imps_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader.paras.label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe \n",
    "gene_names = reader.df_data.columns.tolist()[1:]\n",
    "#{'HRD': 1, 'HR-proficient': 0}\n",
    "#normalise to 0-1\n",
    "d_0 = np_imps_all[0,:,-1]\n",
    "d_0=d_0-d_0.min()/d_0.max()-d_0.min()\n",
    "\n",
    "d_1 = np_imps_all[1,:,-1]\n",
    "d_1=d_1-d_1.min()/d_1.max()-d_1.min()\n",
    "\n",
    "# create dataframe\n",
    "data_dict = {\n",
    "    'gene_names': gene_names,\n",
    "    \"HR-proficient\":d_0, #normalise to 0-1\n",
    "    \"HRD\":d_1,\n",
    "}\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"imp_score_avg.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
